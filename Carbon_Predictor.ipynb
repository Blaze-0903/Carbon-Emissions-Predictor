{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnRTXeoyi9Lj",
        "outputId": "217e7ec3-737a-43a8-b619-845ce6ba995c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Error: 'owid-co2-data.csv' not found. Please ensure the file is in the correct directory.\n",
            "‚úÖ File downloaded and saved as 'owid-co2-data.csv'\n",
            "‚úÖ Relevant columns selected and data sorted by country and year.\n",
            "‚úÖ Missing values interpolated within each country group.\n",
            "‚úÖ Lagged features created for GHG, CO2, Methane, and Nitrous Oxide.\n",
            "‚úÖ Dropped 18606 rows with NaN values after lagging.\n",
            "‚úÖ Features (X) and target (y) defined.\n",
            "‚úÖ Data split into training and testing sets.\n",
            "‚úÖ Features scaled using StandardScaler.\n",
            "‚úÖ XGBoost Regressor model trained.\n",
            "\n",
            "--- Model Evaluation ---\n",
            "‚úÖ R¬≤ Score: 0.9338\n",
            "‚úÖ MAE: 30.91\n",
            "‚úÖ RMSE: 533.95\n",
            "üìÅ Preprocessed data saved as: preprocessed_data.csv\n",
            "üìÅ Model saved as: ghg_model.pkl\n",
            "üìÅ Scaler saved as: ghg_scaler.pkl\n",
            "\n",
            "--- Setup Complete ---\n",
            "You can now use these files with your Streamlit app.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "# --- Configuration ---\n",
        "DATA_FILE = 'owid-co2-data.csv'\n",
        "PREPROCESSED_DATA_FILE = 'preprocessed_data.csv'\n",
        "MODEL_FILE = 'ghg_model.pkl'\n",
        "SCALER_FILE = 'ghg_scaler.pkl'\n",
        "\n",
        "# --- Step 1: Load the dataset ---\n",
        "try:\n",
        "    df = pd.read_csv(DATA_FILE)\n",
        "    print(f\"‚úÖ Dataset '{DATA_FILE}' loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: '{DATA_FILE}' not found. Please ensure the file is in the correct directory.\")\n",
        "    # Fallback to downloading if not found\n",
        "    url = \"https://github.com/owid/co2-data/raw/master/owid-co2-data.csv\"\n",
        "    df = pd.read_csv(url)\n",
        "    df.to_csv(DATA_FILE, index=False)\n",
        "    print(f\"‚úÖ File downloaded and saved as '{DATA_FILE}'\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå An error occurred while loading the data: {e}\")\n",
        "    exit() # Exit if data cannot be loaded\n",
        "\n",
        "# --- Step 2: Select relevant columns and sort data ---\n",
        "# We need country, year, population, gdp, and various emissions types\n",
        "# for total_ghg prediction.\n",
        "columns_to_keep = [\n",
        "    'country', 'year', 'population', 'gdp',\n",
        "    'total_ghg', 'co2', 'methane', 'nitrous_oxide'\n",
        "]\n",
        "df_processed = df[columns_to_keep].copy()\n",
        "df_processed = df_processed.sort_values(by=['country', 'year']).reset_index(drop=True)\n",
        "print(\"‚úÖ Relevant columns selected and data sorted by country and year.\")\n",
        "\n",
        "# --- Step 3: Handle missing values ---\n",
        "# Interpolate missing values within each country group\n",
        "for col in ['total_ghg', 'co2', 'methane', 'nitrous_oxide', 'population', 'gdp']:\n",
        "    df_processed[col] = df_processed.groupby('country')[col].transform(lambda x: x.interpolate(method='linear', limit_direction='both'))\n",
        "print(\"‚úÖ Missing values interpolated within each country group.\")\n",
        "\n",
        "# --- Step 4: Feature Engineering ---\n",
        "# Create lagged features for total_ghg, co2, methane, and nitrous_oxide\n",
        "# Lag features are crucial for time-series prediction\n",
        "lag_features = ['total_ghg', 'co2', 'methane', 'nitrous_oxide']\n",
        "for feature in lag_features:\n",
        "    for i in range(1, 4): # Create lags for 1, 2, and 3 years\n",
        "        df_processed[f'{feature}_lag{i}'] = df_processed.groupby('country')[feature].shift(i)\n",
        "print(\"‚úÖ Lagged features created for GHG, CO2, Methane, and Nitrous Oxide.\")\n",
        "\n",
        "# Drop rows that have NaN values due to lagging (these will be the first few years for each country)\n",
        "initial_rows_count = len(df_processed)\n",
        "df_processed.dropna(inplace=True)\n",
        "df_processed = df_processed.reset_index(drop=True)\n",
        "print(f\"‚úÖ Dropped {initial_rows_count - len(df_processed)} rows with NaN values after lagging.\")\n",
        "\n",
        "# --- Step 5: Define features (X) and target (y) ---\n",
        "# The target variable is 'total_ghg'\n",
        "target_column = 'total_ghg'\n",
        "\n",
        "# Define input features for the model.\n",
        "# These match the 'input_features' list in your original app.py.\n",
        "input_features = [\n",
        "    'population', 'gdp',\n",
        "    'total_ghg_lag1', 'total_ghg_lag2', 'total_ghg_lag3',\n",
        "    'co2_lag1', 'methane_lag1', 'nitrous_oxide_lag1'\n",
        "    # 'year' can also be added as a feature, but for simplicity and to match original app.py,\n",
        "    # we'll use the ones listed in its input_features.\n",
        "    # Note: 'total_ghg' was in the original app.py's input_features, but it's the target.\n",
        "    # We should exclude it from X. If it was meant as a 'current year' feature, it would be data leakage.\n",
        "]\n",
        "\n",
        "# Ensure all defined input features exist in the DataFrame\n",
        "missing_features = [f for f in input_features if f not in df_processed.columns]\n",
        "if missing_features:\n",
        "    print(f\"‚ùå Error: Missing features in preprocessed data: {missing_features}. Please check feature engineering steps.\")\n",
        "    exit()\n",
        "\n",
        "X = df_processed[input_features]\n",
        "y = df_processed[target_column]\n",
        "\n",
        "print(\"‚úÖ Features (X) and target (y) defined.\")\n",
        "\n",
        "# --- Step 6: Split data into training and testing sets ---\n",
        "# Using a simple train-test split for model training\n",
        "# For time-series data, a time-based split is often better, but for general country prediction\n",
        "# and to align with the original app's logic, a random split is used here.\n",
        "# You might consider using TimeSeriesSplit if you want to strictly evaluate forecasting ability.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=df_processed['country'] if 'country' in df_processed.columns else None)\n",
        "# Stratify by country to ensure each country's data is proportionally represented in train/test sets.\n",
        "# Remove stratify if 'country' is not a column in X (which it shouldn't be for the model input).\n",
        "print(\"‚úÖ Data split into training and testing sets.\")\n",
        "\n",
        "# --- Step 7: Scale the features ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"‚úÖ Features scaled using StandardScaler.\")\n",
        "\n",
        "# --- Step 8: Initialize and train the XGBoost Regressor model ---\n",
        "# Using XGBoost as it generally performs well and was used in our previous iterations.\n",
        "model = XGBRegressor(objective='reg:squarederror',\n",
        "                     n_estimators=1000,\n",
        "                     learning_rate=0.05,\n",
        "                     max_depth=5,\n",
        "                     subsample=0.7,\n",
        "                     colsample_bytree=0.7,\n",
        "                     random_state=42,\n",
        "                     n_jobs=-1)\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "print(\"‚úÖ XGBoost Regressor model trained.\")\n",
        "\n",
        "# --- Step 9: Evaluate the model ---\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(f\"\\n--- Model Evaluation ---\")\n",
        "print(f\"‚úÖ R¬≤ Score: {r2:.4f}\")\n",
        "print(f\"‚úÖ MAE: {mae:.2f}\")\n",
        "print(f\"‚úÖ RMSE: {rmse:.2f}\")\n",
        "\n",
        "# --- Step 10: Save the preprocessed data, model, and scaler ---\n",
        "try:\n",
        "    df_processed.to_csv(PREPROCESSED_DATA_FILE, index=False)\n",
        "    print(f\"üìÅ Preprocessed data saved as: {PREPROCESSED_DATA_FILE}\")\n",
        "\n",
        "    joblib.dump(model, MODEL_FILE)\n",
        "    print(f\"üìÅ Model saved as: {MODEL_FILE}\")\n",
        "\n",
        "    joblib.dump(scaler, SCALER_FILE)\n",
        "    print(f\"üìÅ Scaler saved as: {SCALER_FILE}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå An error occurred while saving assets: {e}\")\n",
        "\n",
        "print(\"\\n--- Setup Complete ---\")\n",
        "print(\"You can now use these files with your Streamlit app.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(model, MODEL_FILE)\n",
        "print(f\"üìÅ Model saved as: {MODEL_FILE}\")\n",
        "\n",
        "joblib.dump(scaler, SCALER_FILE)\n",
        "print(f\"üìÅ Scaler saved as: {SCALER_FILE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLo3VJrZkYsn",
        "outputId": "5ee16fd0-ee51-4cae-de0d-1d83775eff7f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Model saved as: ghg_model.pkl\n",
            "üìÅ Scaler saved as: ghg_scaler.pkl\n"
          ]
        }
      ]
    }
  ]
}